#!/usr/bin/env python3
"""
Performance Optimizer for Qwen-Image Generator
Diagnoses and fixes 500+ second generation times on high-end hardware

For AMD Threadripper PRO 5995WX + RTX 4080 + 128GB RAM
Target: Reduce from 500+s to 15-60s per image
"""

import os
import sys
import time
from typing import Dict, List

import psutil
import torch

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def check_system_resources() -> Dict[str, any]:
    """Check system resources and identify bottlenecks"""

    print("üîç SYSTEM RESOURCE ANALYSIS")
    print("=" * 50)

    results = {}

    # CPU Information
    cpu_count = os.cpu_count()
    cpu_freq = psutil.cpu_freq()
    cpu_percent = psutil.cpu_percent(interval=1)

    print("CPU:")
    print(f"  ‚Ä¢ Cores: {cpu_count}")
    print(
        f"  ‚Ä¢ Frequency: {cpu_freq.current:.0f} MHz"
        if cpu_freq
        else "  ‚Ä¢ Frequency: Unknown"
    )
    print(f"  ‚Ä¢ Usage: {cpu_percent:.1f}%")

    results["cpu"] = {
        "cores": cpu_count,
        "frequency": cpu_freq.current if cpu_freq else 0,
        "usage": cpu_percent,
    }

    # Expected: 64 cores, 2700+ MHz
    if cpu_count < 32:
        print(f"  ‚ö†Ô∏è WARNING: Expected 64+ cores, found {cpu_count}")
    else:
        print("  ‚úÖ CPU core count looks good")

    # Memory Information
    memory = psutil.virtual_memory()
    memory_gb = memory.total / 1e9
    memory_available_gb = memory.available / 1e9

    print("\nRAM:")
    print(f"  ‚Ä¢ Total: {memory_gb:.0f}GB")
    print(f"  ‚Ä¢ Available: {memory_available_gb:.0f}GB")
    print(f"  ‚Ä¢ Usage: {memory.percent:.1f}%")

    results["memory"] = {
        "total_gb": memory_gb,
        "available_gb": memory_available_gb,
        "usage_percent": memory.percent,
    }

    # Expected: 128GB+
    if memory_gb < 100:
        print(f"  ‚ö†Ô∏è WARNING: Expected 128GB+ RAM, found {memory_gb:.0f}GB")
    else:
        print("  ‚úÖ RAM amount looks good")

    # GPU Information
    if torch.cuda.is_available():
        device_props = torch.cuda.get_device_properties(0)
        total_vram = device_props.total_memory / 1e9
        allocated_vram = torch.cuda.memory_allocated(0) / 1e9
        available_vram = total_vram - allocated_vram

        print("\nGPU:")
        print(f"  ‚Ä¢ Model: {device_props.name}")
        print(f"  ‚Ä¢ Total VRAM: {total_vram:.1f}GB")
        print(f"  ‚Ä¢ Allocated: {allocated_vram:.1f}GB")
        print(f"  ‚Ä¢ Available: {available_vram:.1f}GB")
        print(f"  ‚Ä¢ Compute Capability: {device_props.major}.{device_props.minor}")

        results["gpu"] = {
            "name": device_props.name,
            "total_vram": total_vram,
            "allocated_vram": allocated_vram,
            "available_vram": available_vram,
            "compute_capability": f"{device_props.major}.{device_props.minor}",
        }

        # Expected: RTX 4080, 16GB VRAM
        if "RTX 4080" not in device_props.name:
            print(f"  ‚ö†Ô∏è WARNING: Expected RTX 4080, found {device_props.name}")

        if total_vram < 15:
            print(f"  ‚ö†Ô∏è WARNING: Expected 16GB VRAM, found {total_vram:.1f}GB")
        else:
            print("  ‚úÖ GPU specs look good")

    else:
        print("\nGPU:")
        print("  ‚ùå CUDA not available!")
        results["gpu"] = None

    return results


def check_pytorch_configuration() -> Dict[str, any]:
    """Check PyTorch configuration for performance issues"""

    print("\nüîç PYTORCH CONFIGURATION ANALYSIS")
    print("=" * 50)

    results = {}

    # PyTorch version
    pytorch_version = torch.__version__
    print(f"PyTorch Version: {pytorch_version}")
    results["pytorch_version"] = pytorch_version

    # CUDA version
    if torch.cuda.is_available():
        cuda_version = torch.version.cuda
        print(f"CUDA Version: {cuda_version}")
        results["cuda_version"] = cuda_version

        # Check CUDA efficiency
        print("\nCUDA Configuration:")
        print(f"  ‚Ä¢ Device Count: {torch.cuda.device_count()}")
        print(f"  ‚Ä¢ Current Device: {torch.cuda.current_device()}")

        # Check for performance-killing settings
        print("\nPerformance Settings:")

        # Check memory fraction
        try:
            # This is approximate - actual check would require internal PyTorch state
            print("  ‚Ä¢ Memory Management: Checking...")
        except:
            pass

        # Check environment variables
        perf_vars = {
            "PYTORCH_CUDA_ALLOC_CONF": "Memory allocation config",
            "CUDA_LAUNCH_BLOCKING": "Synchronous execution",
            "OMP_NUM_THREADS": "CPU threading",
            "MKL_NUM_THREADS": "Intel MKL threading",
        }

        print("\nEnvironment Variables:")
        for var, description in perf_vars.items():
            value = os.environ.get(var, "Not set")
            print(f"  ‚Ä¢ {var}: {value}")

            # Performance recommendations
            if var == "CUDA_LAUNCH_BLOCKING" and value == "1":
                print("    ‚ö†Ô∏è BLOCKING ENABLED - Major performance killer!")
            elif var == "OMP_NUM_THREADS" and (
                value == "Not set" or int(value) if value.isdigit() else 0 < 16
            ):
                print("    üí° Consider setting to 32+ for Threadripper")

    return results


def diagnose_generation_bottlenecks() -> List[str]:
    """Identify specific bottlenecks causing slow generation"""

    print("\nüîç GENERATION BOTTLENECK ANALYSIS")
    print("=" * 50)

    issues = []

    # Check if model is using CPU offloading
    print("Checking common performance killers...")

    # 1. Check for CPU offloading indicators
    if torch.cuda.is_available():
        # Simulate checking if components are being offloaded
        print("  ‚Ä¢ CPU Offloading: Analyzing...")

        # Common signs of CPU offloading:
        # - Sequential CPU offload enabled
        # - Model CPU offload enabled
        # - Device map with CPU assignments

        # This would be detected during model loading
        issues.append(
            "CPU offloading may be enabled - keeps model components on slow CPU"
        )

    # 2. Check memory restrictions
    print("  ‚Ä¢ Memory Restrictions: Analyzing...")

    # Look for conservative memory settings
    config_files = ["src/qwen_edit_config.py", "src/qwen_image_config.py"]

    for config_file in config_files:
        if os.path.exists(config_file):
            with open(config_file, "r") as f:
                content = f.read()

                if "max_memory" in content and '"12GB"' in content:
                    issues.append(
                        f"Conservative VRAM limit in {config_file} - only using 12GB of 16GB"
                    )

                if 'enable_sequential_cpu_offload": True' in content:
                    issues.append(
                        f"Sequential CPU offload enabled in {config_file} - major performance killer"
                    )

                if 'low_cpu_mem_usage": True' in content:
                    issues.append(
                        f"Low CPU memory usage enabled in {config_file} - unnecessary with 128GB RAM"
                    )

    # 3. Check attention slicing
    print("  ‚Ä¢ Attention Slicing: Analyzing...")
    issues.append(
        "Attention slicing may be enabled - trades speed for memory on high-VRAM systems"
    )

    # 4. Check device map issues
    print("  ‚Ä¢ Device Mapping: Analyzing...")
    issues.append("Device mapping may be forcing CPU usage instead of pure GPU")

    return issues


def run_performance_benchmark() -> float:
    """Run a quick performance benchmark"""

    print("\nüß™ PERFORMANCE BENCHMARK")
    print("=" * 50)

    if not torch.cuda.is_available():
        print("‚ùå CUDA not available - cannot benchmark")
        return 0.0

    print("Running GPU compute benchmark...")

    # Clear cache
    torch.cuda.empty_cache()

    # Create test tensors
    device = "cuda"
    dtype = torch.bfloat16

    # Simulate diffusion model operations
    batch_size = 1
    height, width = 928, 1664
    latent_channels = 4

    print(f"Test configuration: {batch_size}x{latent_channels}x{height//8}x{width//8}")

    # Test tensor operations
    start_time = time.time()

    with torch.no_grad():
        # Simulate UNet operations
        latents = torch.randn(
            batch_size,
            latent_channels,
            height // 8,
            width // 8,
            device=device,
            dtype=dtype,
        )

        # Simulate 10 denoising steps
        for step in range(10):
            # Simulate UNet forward pass
            noise_pred = torch.nn.functional.conv2d(
                latents,
                torch.randn(
                    latent_channels, latent_channels, 3, 3, device=device, dtype=dtype
                ),
                padding=1,
            )

            # Simulate scheduler step
            latents = latents - 0.1 * noise_pred

            # Add some compute
            latents = torch.nn.functional.relu(latents)

    torch.cuda.synchronize()
    elapsed_time = time.time() - start_time

    steps_per_second = 10 / elapsed_time
    estimated_50_step_time = 50 / steps_per_second

    print("Benchmark Results:")
    print(f"  ‚Ä¢ 10 steps completed in: {elapsed_time:.2f}s")
    print(f"  ‚Ä¢ Steps per second: {steps_per_second:.2f}")
    print(f"  ‚Ä¢ Estimated 50-step generation: {estimated_50_step_time:.1f}s")

    if estimated_50_step_time > 100:
        print("  ‚ùå POOR performance - should be 15-60s")
    elif estimated_50_step_time > 60:
        print("  ‚ö†Ô∏è SLOW performance - could be better")
    else:
        print("  ‚úÖ GOOD performance")

    return estimated_50_step_time


def generate_optimization_recommendations(
    system_info: Dict, issues: List[str]
) -> List[str]:
    """Generate specific optimization recommendations"""

    print("\nüí° OPTIMIZATION RECOMMENDATIONS")
    print("=" * 50)

    recommendations = []

    # High-level recommendations
    recommendations.append("CRITICAL: Switch to high-end configuration")
    recommendations.append(
        "  ‚Üí Use HighEndQwenImageGenerator instead of QwenImageGenerator"
    )
    recommendations.append("  ‚Üí File: src/qwen_highend_generator.py")

    # Specific fixes based on issues
    for issue in issues:
        if "CPU offload" in issue:
            recommendations.append("DISABLE all CPU offloading:")
            recommendations.append("  ‚Üí Set enable_sequential_cpu_offload = False")
            recommendations.append("  ‚Üí Set enable_model_cpu_offload = False")
            recommendations.append("  ‚Üí Keep entire model on GPU")

        elif "12GB" in issue:
            recommendations.append("INCREASE VRAM allocation:")
            recommendations.append("  ‚Üí Change max_memory from {'0': '12GB'} to None")
            recommendations.append("  ‚Üí Use full 16GB VRAM available")

        elif "low_cpu_mem_usage" in issue:
            recommendations.append("DISABLE low_cpu_mem_usage:")
            recommendations.append("  ‚Üí Set low_cpu_mem_usage = False")
            recommendations.append("  ‚Üí Utilize your 128GB RAM effectively")

        elif "attention slicing" in issue:
            recommendations.append("DISABLE attention slicing:")
            recommendations.append("  ‚Üí Set enable_attention_slicing = False")
            recommendations.append("  ‚Üí Use full VRAM for maximum speed")

    # Hardware-specific recommendations
    if system_info.get("cpu", {}).get("cores", 0) >= 32:
        recommendations.append("OPTIMIZE CPU threading:")
        recommendations.append("  ‚Üí Set OMP_NUM_THREADS=32")
        recommendations.append("  ‚Üí Set MKL_NUM_THREADS=32")

    if system_info.get("gpu", {}).get("total_vram", 0) >= 15:
        recommendations.append("MAXIMIZE VRAM usage:")
        recommendations.append("  ‚Üí Set device_map = None")
        recommendations.append("  ‚Üí Keep all components on GPU")
        recommendations.append("  ‚Üí Set PYTORCH_CUDA_MEMORY_FRACTION=0.95")

    # Environment optimizations
    recommendations.append("SET performance environment variables:")
    recommendations.append("  ‚Üí PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
    recommendations.append("  ‚Üí CUDA_LAUNCH_BLOCKING=0")

    return recommendations


def apply_quick_fixes() -> bool:
    """Apply quick performance fixes"""

    print("\nüîß APPLYING QUICK PERFORMANCE FIXES")
    print("=" * 50)

    try:
        # Set optimal environment variables
        optimal_env = {
            "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:True,max_split_size_mb:512",
            "CUDA_LAUNCH_BLOCKING": "0",
            "OMP_NUM_THREADS": "32",
            "MKL_NUM_THREADS": "32",
            "NUMBA_NUM_THREADS": "32",
        }

        for key, value in optimal_env.items():
            os.environ[key] = value
            print(f"‚úÖ Set {key}={value}")

        # Set CUDA memory fraction
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(0.95)
            print("‚úÖ Set CUDA memory fraction to 95%")

        # Clear GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("‚úÖ Cleared GPU cache")

        print("\n‚úÖ Quick fixes applied!")
        print("‚ö†Ô∏è For permanent fixes, update configuration files")

        return True

    except Exception as e:
        print(f"‚ùå Error applying fixes: {e}")
        return False


def main():
    """Main performance optimization workflow"""

    print("üöÄ QWEN-IMAGE PERFORMANCE OPTIMIZER")
    print("=" * 60)
    print("Diagnosing 500+ second generation times...")
    print("Target: 15-60 seconds on high-end hardware")
    print("=" * 60)

    # 1. Check system resources
    system_info = check_system_resources()

    # 2. Check PyTorch configuration
    pytorch_info = check_pytorch_configuration()

    # 3. Diagnose bottlenecks
    issues = diagnose_generation_bottlenecks()

    # 4. Run benchmark
    benchmark_time = run_performance_benchmark()

    # 5. Generate recommendations
    recommendations = generate_optimization_recommendations(system_info, issues)

    # Print summary
    print("\nüìä ANALYSIS SUMMARY")
    print("=" * 50)

    print(f"Issues Found: {len(issues)}")
    for issue in issues:
        print(f"  ‚Ä¢ {issue}")

    print("\nRecommendations:")
    for rec in recommendations:
        print(f"  {rec}")

    # 6. Offer to apply quick fixes
    print("\nüîß QUICK FIXES AVAILABLE")
    print("Apply immediate environment optimizations? (y/n): ", end="")

    try:
        response = input().lower().strip()
        if response in ["y", "yes"]:
            success = apply_quick_fixes()
            if success:
                print("\n‚úÖ Quick fixes applied!")
                print("‚ö†Ô∏è Restart your application to see full benefits")
        else:
            print("Skipping quick fixes")
    except KeyboardInterrupt:
        print("\nOperation cancelled")

    print("\nüéØ NEXT STEPS:")
    print("1. Switch to high-end generator: src/qwen_highend_generator.py")
    print("2. Update configuration files with recommendations")
    print("3. Restart application")
    print("4. Monitor generation times (target: 15-60s)")

    return benchmark_time


if __name__ == "__main__":
    main()
